{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize #import word_tokenize for tokenizing text into words \n",
    "from nltk.tokenize import sent_tokenize #import sent_tokenize for tokenizing paragraph into sentences\n",
    "from nltk.stem.porter import PorterStemmer #import Porter Stemmer Algorithm \n",
    "from nltk.stem import WordNetLemmatizer #import WordNet lemmatizer \n",
    "from nltk.corpus import stopwords #import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #import Indonesian Stemmer\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import sys\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file\n",
    "def input_text(file):\n",
    "    f = open(file,'r')\n",
    "    isifile = f.readlines()\n",
    "    isifile = ''.join(isifile)\n",
    "    return isifile\n",
    "\n",
    "# sentence tokenization\n",
    "def sentence_tokenization(s):\n",
    "    sentences_list = sent_tokenize(s)\n",
    "    sentences_list = \" \".join(sentences_list)\n",
    "    return sentences_list\n",
    "\n",
    "# stopword remover\n",
    "def stopword_re(s):\n",
    "    factory = StopWordRemoverFactory()\n",
    "    other_stopword = ['sendiri', 'ini', 'amat'] # sudah ditambahan tapi tidak terhapus \n",
    "    data = factory.get_stop_words()+other_stopword\n",
    "    stopword = factory.create_stop_word_remover()\n",
    "    text_stopword = stopword.remove(s)\n",
    "    return text_stopword\n",
    "\n",
    "#casefolding\n",
    "def casefolding(s):\n",
    "    new_str = s.lower()\n",
    "    return new_str\n",
    "\n",
    "#remove from string\n",
    "def remove(str):\n",
    "    new_string =  re.sub(r\"[\\n\\.]\", \" \", str)\n",
    "    return new_string\n",
    "\n",
    "# Stemming\n",
    "def stemmingIndo(str):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(str)\n",
    "\n",
    "# word tokenization\n",
    "def word_tokenization(s):\n",
    "    token2 = word_tokenize(s)\n",
    "    return token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bakar', 'hutan', 'jadi', 'indonesia', 'indonesia', 'salah', 'satu', 'negara', 'dalam', 'milik', 'iklim', 'tropis', 'negara', 'diri', 'banyak', 'pulau', 'tiap', 'wilayah', 'meski', 'aku', 'indonesia', 'sendiri', 'datar', 'luas', 'laut', 'bentang', 'luas', 'namun', 'demikian', 'indonesia', 'milik', 'kawasan', 'hutan', 'cukup', 'banyak', 'mulai', 'sabang', 'letak', 'provinsi', 'aceh', 'merauke', 'letak', 'kawasan', 'papua', 'namun', 'beberapa', 'tahun', 'akhir', 'indonesia', 'sering', 'alami', 'bakar', 'hutan', 'lantar', 'beberapa', 'faktor', 'ada', 'mulai', 'faktor', 'buat', 'manusia', 'sendiri', 'hingga', 'faktor', 'alam', 'faktor', 'alam', 'sebab', 'bakar', 'hutan', 'memang', 'hindar', 'ada', 'salah', 'ini', 'akan', 'tetapi', 'faktor', 'tindak', 'manusia', 'perlu', 'tindak', 'evaluasi', 'memang', 'buah', 'resah', 'sendiri', 'manusia', 'banyak', 'kini', 'hilang', 'sadar', 'sampai', 'laku', 'suatu', 'buat', 'bisa', 'rugi', 'banyak', 'orang', 'masuk', 'diri', 'sendiri', 'khusus', 'rugi', 'lingkung', 'hidup', 'sedang', 'hutan', 'sendiri', 'jenis', 'habitat', 'dalam', 'banyak', 'spesies', 'gantung', 'oleh', 'itu', 'aksi', 'manusia', 'sebab', 'bakar', 'hutan', 'adil', 'lebih', 'tuju', 'penting', 'diri', 'sendiri', 'ada', 'banyak', 'alas', 'milik', 'oknum', 'laku', 'aksi', 'bakar', 'hutan', 'di', 'antara', 'buka', 'lahan', 'baru', 'bangun', 'gedung', 'baru', 'lain', 'akan', 'tetapi', 'sama', 'sekali', 'pikir', 'bagaimana', 'nasib', 'flora', 'fauna', 'ada', 'di', 'dalam', 'hutan', 'sebut', 'flora', 'fauna', 'dapat', 'di', 'dalam', 'hutan', 'lari', 'diri', 'namun', 'ada', 'juga', 'hangus', 'bakar', 'api', 'lantar', 'ulah', 'dari', 'manusia', 'itu', 'sendiri', 'mereka', 'akan', 'hilang', 'tempat', 'tinggal', 'asli', 'bahkan', 'akan', 'jadi', 'resah', 'sendiri', 'mereka', 'masuk', 'wilayah', 'mukim', 'duduk', 'asa', 'tidak', 'milik', 'rumah', 'untuk', 'tinggal', 'maka', 'tidak', 'heran', 'akhir', 'ada', 'banyak', 'kasus', 'temu', 'hewan', 'liar', 'singa', 'macan', 'masuk', 'mukim', 'warga', 'beda', 'faktor', 'alam', 'misal', 'kemarau', 'panjang', 'ada', 'sambar', 'petir', 'kala', 'hujan', 'datang', 'musim', 'tidak', 'bisa', 'kira', 'manusia', 'saat', 'kemarau', 'datang', 'dengan', 'masa', 'amat', 'panjang', 'hal', 'sangat', 'wajar', 'akan', 'tetapi', 'hal', 'sebut', 'sangat', 'pengaruh', 'kondisi', 'hutan', 'yang', 'tiap', 'hari', 'kena', 'sengat', 'matahari', 'sebab', 'muncul', 'percik', 'api', 'juga', 'ada', 'petir', 'yang', 'sambar', 'muncul', 'percik', 'api']\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    teks = input_text('artikelindo.txt')\n",
    "    indo_token = sentence_tokenization(teks)\n",
    "    indo_stopword = stopword_re(indo_token)\n",
    "    indo_case = casefolding(indo_stopword)\n",
    "    indo_remove = remove(indo_case)\n",
    "    indo_stemming = stemmingIndo(indo_case)\n",
    "    indo_token2 = word_tokenization(indo_stemming)\n",
    "    print(indo_token2)\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
