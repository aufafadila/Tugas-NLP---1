{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize #import word_tokenize for tokenizing text into words \n",
    "from nltk.tokenize import sent_tokenize #import sent_tokenize for tokenizing paragraph into sentences\n",
    "from nltk.stem.porter import PorterStemmer #import Porter Stemmer Algorithm \n",
    "from nltk.stem import WordNetLemmatizer #import WordNet lemmatizer \n",
    "from nltk.corpus import stopwords #import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file\n",
    "def input_file(file):\n",
    "    f = open(file,'r', encoding='utf8')\n",
    "    txt_file = f.read()\n",
    "    return txt_file\n",
    "\n",
    "# sentence tokenization\n",
    "def sentence_tokenization(s):\n",
    "    sentences_list = sent_tokenize(s)\n",
    "    sentences_list = \" \".join(sentences_list)\n",
    "    return sentences_list\n",
    "\n",
    "# remove from string\n",
    "def remove(str):\n",
    "    new_string =  re.sub(r\"[^A-Za-z]\", \" \", str)\n",
    "    return new_string\n",
    "\n",
    "# casefolding\n",
    "def casefolding(s):\n",
    "    new_str = s.lower()\n",
    "    return new_str\n",
    "\n",
    "# word tokenization\n",
    "def word_tokenization(s):\n",
    "    token2 = word_tokenize(s)\n",
    "    return token2\n",
    "\n",
    "# stopword\n",
    "def stopword(s):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = word_tokenization(s)\n",
    "    wordsFiltered = []\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    wordsFiltered = \" \".join(wordsFiltered)\n",
    "    return wordsFiltered\n",
    "\n",
    "# Stemming English\n",
    "def stemmingEnglish(str):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = word_tokenization(str)\n",
    "    result = list()\n",
    "    for word in words:\n",
    "        result.append(porter_stemmer.stem(word))\n",
    "    return ' '.join(result)\n",
    "\n",
    "#pos tagging\n",
    "def postag(str):\n",
    "    tok_sentence = word_tokenization(str)\n",
    "    tagged_sentence = nltk.pos_tag(tok_sentence)\n",
    "    return tagged_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('teenag', 'NN'), ('use', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('find', 'VBP'), ('commun', 'JJ'), ('mental', 'JJ'), ('health', 'NN'), ('often', 'RB'), ('neg', 'JJ'), ('affect', 'VBP'), ('cultur', 'JJ'), ('comparison', 'NN'), ('verdict', 'NN'), ('still', 'RB'), ('whether', 'IN'), ('social', 'JJ'), ('media', 'NNS'), ('damag', 'VBP'), ('mental', 'JJ'), ('health', 'NN'), ('teen', 'JJ'), ('part', 'NN'), ('due', 'JJ'), ('lack', 'NN'), ('research', 'NN'), ('studi', 'NN'), ('show', 'NN'), ('onlin', 'IN'), ('connect', 'JJ'), ('small', 'JJ'), ('group', 'NN'), ('peopl', 'NN'), ('benefici', 'NN'), ('teen', 'JJ'), ('research', 'NN'), ('point', 'NN'), ('rise', 'NN'), ('symptom', 'NN'), ('anxieti', 'NN'), ('depress', 'NN'), ('eat', 'NN'), ('disord', 'NN'), ('reason', 'NN'), ('difficult', 'JJ'), ('get', 'VB'), ('good', 'JJ'), ('read', 'NN'), ('issu', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('constantli', 'VBP'), ('chang', 'NN'), ('evolv', 'NN'), ('plu', 'NN'), ('long', 'JJ'), ('term', 'NN'), ('studi', 'NN'), ('complet', 'NN'), ('left', 'VBD'), ('make', 'VBP'), ('educ', 'JJ'), ('guess', 'NN'), ('base', 'NN'), ('current', 'JJ'), ('research', 'NN'), ('enough', 'NN'), ('data', 'NNS'), ('back', 'RB'), ('potenti', 'VBP'), ('long', 'JJ'), ('term', 'NN'), ('pro', 'JJ'), ('con', 'FW'), ('live', 'VBP'), ('like', 'IN'), ('small', 'JJ'), ('studi', 'NN'), ('worrisom', 'NN'), ('result', 'NN'), ('one', 'CD'), ('studi', 'NN'), ('univers', 'NNS'), ('pittsburgh', 'VBP'), ('exampl', 'RB'), ('found', 'VBN'), ('correl', 'JJ'), ('time', 'NN'), ('spent', 'VBN'), ('scroll', 'RB'), ('social', 'JJ'), ('media', 'NNS'), ('app', 'VBP'), ('neg', 'JJ'), ('bodi', 'NN'), ('imag', 'JJ'), ('feedback', 'NN'), ('spent', 'JJ'), ('time', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('time', 'NN'), ('risk', 'NN'), ('report', 'NN'), ('eat', 'VBD'), ('bodi', 'JJ'), ('imag', 'NN'), ('concern', 'NN'), ('compar', 'JJ'), ('peer', 'NN'), ('spent', 'VBN'), ('less', 'JJR'), ('time', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('particip', 'JJ'), ('spent', 'JJ'), ('time', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('time', 'NN'), ('risk', 'NN'), ('result', 'NN'), ('separ', 'NN'), ('studi', 'NN'), ('univers', 'NNS'), ('pittsburgh', 'VBP'), ('school', 'NN'), ('medicin', 'NN'), ('show', 'NN'), ('time', 'NN'), ('young', 'JJ'), ('adult', 'NN'), ('spent', 'VBN'), ('social', 'JJ'), ('media', 'NNS'), ('like', 'IN'), ('problem', 'NN'), ('sleep', 'VBP'), ('report', 'NN'), ('symptom', 'NN'), ('depress', 'IN'), ('anoth', 'DT'), ('small', 'JJ'), ('studi', 'NN'), ('teen', 'JJ'), ('age', 'NN'), ('ucla', 'JJ'), ('brain', 'NN'), ('map', 'NN'), ('center', 'NN'), ('found', 'VBD'), ('receiv', 'RB'), ('high', 'JJ'), ('number', 'NN'), ('like', 'IN'), ('photo', 'NN'), ('show', 'NN'), ('increas', 'JJ'), ('activ', 'RB'), ('reward', 'VBP'), ('center', 'NN'), ('brain', 'NN'), ('teen', 'JJ'), ('influenc', 'NN'), ('like', 'IN'), ('photo', 'NN'), ('regardless', 'RB'), ('content', 'JJ'), ('base', 'NN'), ('high', 'JJ'), ('number', 'NN'), ('like', 'IN'), ('bottom', 'JJ'), ('line', 'NN'), ('feel', 'VB'), ('good', 'JJ'), ('like', 'IN'), ('herd', 'NN'), ('mental', 'JJ'), ('big', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('like', 'IN'), ('other', 'JJ'), ('like', 'IN'), ('upsid', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('teen', 'JJ'), ('posit', 'NN'), ('aspect', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('import', 'NN'), ('rememb', 'VBP'), ('teen', 'JJ'), ('hardwir', 'JJ'), ('social', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('make', 'VBP'), ('social', 'JJ'), ('easi', 'NN'), ('immedi', 'NN'), ('teen', 'JJ'), ('struggl', 'JJ'), ('social', 'JJ'), ('skill', 'NN'), ('social', 'JJ'), ('anxieti', 'NN'), ('easi', 'JJ'), ('access', 'NN'), ('face', 'NN'), ('face', 'VBP'), ('social', 'JJ'), ('teen', 'NN'), ('might', 'MD'), ('benefit', 'VB'), ('connect', 'JJ'), ('teen', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('teen', 'JJ'), ('margin', 'NN'), ('group', 'NN'), ('includ', 'VBZ'), ('lgbtq', 'RB'), ('teen', 'JJ'), ('teen', 'JJ'), ('struggl', 'JJ'), ('mental', 'JJ'), ('health', 'NN'), ('issu', 'NN'), ('find', 'VBP'), ('support', 'NN'), ('friendship', 'NN'), ('use', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('teen', 'JJ'), ('connect', 'VBP'), ('small', 'JJ'), ('group', 'NN'), ('support', 'NN'), ('teen', 'NN'), ('via', 'IN'), ('social', 'JJ'), ('media', 'NNS'), ('connect', 'VBP'), ('differ', 'VBP'), ('live', 'JJ'), ('isol', 'NN'), ('find', 'VB'), ('support', 'NN'), ('downsid', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('use', 'VBP'), ('teen', 'JJ'), ('read', 'VBN'), ('enough', 'JJ'), ('current', 'JJ'), ('research', 'NN'), ('find', 'NN'), ('neg', 'JJ'), ('tend', 'VBP'), ('feel', 'VB'), ('bigger', 'JJR'), ('posit', 'NN'), ('teen', 'NN'), ('use', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('connect', 'VBP'), ('creat', 'NN'), ('friendship', 'NN'), ('other', 'JJ'), ('also', 'RB'), ('confront', 'VBP'), ('cyberbulli', 'JJ'), ('troll', 'NN'), ('toxic', 'NN'), ('comparison', 'NN'), ('sleep', 'JJ'), ('depriv', 'NN'), ('less', 'RBR'), ('frequent', 'JJ'), ('face', 'NN'), ('face', 'NN'), ('interact', 'VB'), ('name', 'RB'), ('much', 'JJ'), ('time', 'NN'), ('spent', 'VBN'), ('scroll', 'RB'), ('social', 'JJ'), ('media', 'NNS'), ('result', 'VBP'), ('symptom', 'JJ'), ('anxieti', 'NN'), ('depress', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('destruct', 'VBP'), ('focus', 'NN'), ('like', 'IN'), ('need', 'VBP'), ('gain', 'VBP'), ('like', 'IN'), ('social', 'JJ'), ('media', 'NNS'), ('caus', 'VBP'), ('teen', 'JJ'), ('make', 'NN'), ('choic', 'NN'), ('would', 'MD'), ('otherwis', 'VB'), ('make', 'VB'), ('includ', 'JJ'), ('alter', 'NN'), ('appear', 'VBP'), ('engag', 'JJ'), ('neg', 'JJ'), ('behavior', 'NN'), ('accept', 'IN'), ('riski', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('challeng', 'VBP'), ('cyberbulli', 'NN'), ('teen', 'JJ'), ('girl', 'NN'), ('particular', 'JJ'), ('risk', 'NN'), ('cyberbulli', 'NN'), ('use', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('teen', 'JJ'), ('boy', 'JJ'), ('immun', 'NN'), ('cyberbulli', 'NN'), ('associ', 'NN'), ('depress', 'NN'), ('anxieti', 'NN'), ('elev', 'NN'), ('risk', 'NN'), ('suicid', 'NN'), ('thought', 'VBD'), ('make', 'VB'), ('comparison', 'NN'), ('though', 'IN'), ('mani', 'JJ'), ('teen', 'NN'), ('know', 'VBP'), ('peer', 'NN'), ('share', 'NN'), ('highlight', 'JJ'), ('reel', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('difficult', 'JJ'), ('avoid', 'JJ'), ('make', 'NN'), ('comparison', 'NN'), ('everyth', 'NN'), ('physic', 'JJ'), ('appear', 'JJ'), ('life', 'NN'), ('circumst', 'NN'), ('perceiv', 'NN'), ('success', 'NN'), ('failur', 'NN'), ('microscop', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('mani', 'JJ'), ('fake', 'VBP'), ('friend', 'VB'), ('even', 'RB'), ('privaci', 'JJ'), ('set', 'VBN'), ('place', 'NN'), ('teen', 'JJ'), ('collect', 'JJ'), ('thousand', 'CD'), ('friend', 'NN'), ('friend', 'NN'), ('friend', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('peopl', 'VBP'), ('friend', 'JJ'), ('list', 'NN'), ('peopl', 'NN'), ('access', 'NN'), ('screenshot', 'JJ'), ('photo', 'NN'), ('snap', 'NN'), ('updat', 'JJ'), ('use', 'NN'), ('purpos', 'NN'), ('privaci', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('less', 'RBR'), ('face', 'JJ'), ('time', 'NN'), ('social', 'JJ'), ('interact', 'NN'), ('skill', 'JJ'), ('requir', 'VBZ'), ('daili', 'VBP'), ('practic', 'JJ'), ('even', 'RB'), ('teen', 'JJ'), ('difficult', 'JJ'), ('build', 'NN'), ('empathi', 'JJ'), ('compass', 'NN'), ('best', 'JJS'), ('weapon', 'NN'), ('war', 'NN'), ('bulli', 'NN'), ('teen', 'JJ'), ('spend', 'JJ'), ('time', 'NN'), ('engag', 'NN'), ('onlin', 'IN'), ('person', 'NN'), ('human', 'JJ'), ('connect', 'NN'), ('power', 'NN'), ('tool', 'NN'), ('build', 'NN'), ('skill', 'NN'), ('last', 'JJ'), ('lifetim', 'JJ'), ('happi', 'NN'), ('medium', 'NN'), ('somewher', 'NN'), ('key', 'NN'), ('help', 'NN'), ('teen', 'VB'), ('learn', 'JJ'), ('balanc', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('real', 'JJ'), ('life', 'NN'), ('friendship', 'NN'), ('keep', 'VB'), ('line', 'NN'), ('commun', 'NN'), ('open', 'JJ'), ('keep', 'VB'), ('talk', 'NN'), ('honest', 'JJS'), ('commun', 'NN'), ('show', 'NN'), ('teen', 'JJ'), ('support', 'NN'), ('judg', 'NN'), ('lectur', 'NN'), ('also', 'RB'), ('import', 'NN'), ('walk', 'NN'), ('walk', 'VBP'), ('disconnect', 'JJ'), ('weekend', 'NN'), ('show', 'NN'), ('teen', 'JJ'), ('whole', 'JJ'), ('world', 'NN'), ('requir', 'NN'), ('handheld', 'VBD'), ('screen', 'NN'), ('may', 'MD'), ('miss', 'VB'), ('phone', 'NN'), ('lot', 'NN'), ('less', 'JJR'), ('think', 'VBP'), ('good', 'JJ'), ('lesson', 'NN'), ('learn', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    text = input_file('artikeling.txt')\n",
    "    text_token = sentence_tokenization(text)\n",
    "    text_remove = remove(text_token)\n",
    "    text_case = casefolding(text_remove)\n",
    "    text_stopword = stopword(text_case)\n",
    "    text_stemming = stemmingEnglish(text_stopword)\n",
    "    text_postag = postag(text_stemming)\n",
    "    print(text_postag)\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
